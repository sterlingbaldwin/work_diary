# Week of 8/13/2018


Found and identified an issue stopping the processflow from running correctly on rhea and anvil. Turns out the ssh library I use (paramiko) has an issue on some systems where it will get stuck in deadlock at import time.
This was a particularly thorny issue to deal with as nothing I could do would stop it, and it would get stuck during the import step and never finish. I did a short-term resolve of this by changing the filemanager to dynamically load
the transport interface requested by the user in the config, so the libraries will only be loaded if the user requests ssh file transport. The upside of this is that the import step is now much faster in some configurations, since
globus was one of the slowest to load modules.

Responded to IPPP personel feedback requests.

Discovered that sometimes, some jobs would fail on anvil. Very confusingly the error given would be that the job couldnt access my home directory, even though nothing in the run script or any of the jobs commands touched my home directory, or did anything outside of the bulk storage area. Contacted LCRC support, and was informed that all jobs create temporary out/error log files in the users home directory, even when the log/error flags have been explicitly set to go somewhere else. The logs are just moved to the requested location after the jobs finishes. They commented that the -D flag to change the jobs running directory can sometimes cause undefined behavior and suggested I remove it. Editorial: slurm is such a rediculusly better job scheduler then PBS it astonishes me when I come across computing centers that still use it. Updated the affected job types, after testing this seems to have solved the problem. All jobs are now working at LCRC, NERSC, and LLNL. After talking to the LCRC support staff it seems they agree with me, the blues cluster is apparently going to switch over to slurm sometime in the next couple months, so Im not going to invest any additional time/effort into dealing with PBS related issues.

Switched from the 1.2.0 version of the e3sm-unified to 1.2.1 for all jobs, this seems to have fixed the land regridding issue that showed up in nco 4.7.4, new version runs 4.7.6 which has the bug fix.

Merged changes into the master branch after testing, updated all the version numbers to 2.0.3 and rebuilt/released to the anaconda e3sm channel. Working with Xylar to get the package back into the e3sm-unified now that the conflicting depencies have rebuilt using the latest gcc.

A user reported that when she started a run with over 1400 remote files, the processflow would throw an unexpected error. Upon investigation I found that sqllite (which the pflow uses to store file information) has a hard cap on the number of records that can be present in a given transation, so when I was trying to update all the records at once it would crash and burn.
Once I had found and diagnosed the problem it was a fairly straightforwad fix, simply iterate over the records in steps of less then the cap. Takes a little longer, but its about .5 seconds to do the update instead of 0.1 which I dont see as a big issue.

The same user gave me a copy of her run config, which contained some corner cases I hadnt thought of. I added additional checks so that if the user was requesting year set lengths that didnt mod evenly with the start and end years, the processflow would truncate and do a best-effort attempt to comply with the users request even if it didnt totally make sense. This will stop of from crashing when given bad input, but also stop the creation of potentiall duplicate jobs.

After rebuilding and releasing the new minor version (2.0.4), I wiped all my conda environments and did a reinstall and end to end test run on acme1, edison, and anvil. Everything installed fine, and the jobs went fine except an error running aprime on blues. It looks like it was just a max-time-limit error, so tomorrow morning I'll retry after doubling the time limit. The acme test was a complete end to end run with 29 years of data, transfering from edison, running all jobs at 5, 10, 20 year lengths. Everything went fine, all jobs finished and the output was hosted successfully.

I ran the same set of jobs on edison, but discovered a vexing bug. For some reason when run here all the amwg jobs produced the correct output, but the amwg jobs post-completion handler wasnt able to find the output, reporting "plots at location .... were not found."  When I when to check, all the output was there. Something weird happened where the processflow wasnt seeing the output as being there after the job had finished, and so wasnt able to move it properly to the host location.

A bit of poking around in the debugger has revieled that plotset 16 simply does not play well on edison. I have no godly clue why this is, but since Ive never heard any scientist refer to using plotset 16, and many of the variables it tries to compare to are simply not included in e3sm model output, I just turned it off in the amwg template. This seems to have solved the problem. If any scientstis complains, I can add to the documentation how to make changes the the amwg template but I doubt this will happen.

A couple more e3sm scientists expressed interest in using the processflow to run their diagnostics on acme1, so Im working with Tony to get them setup, and pointed them to all the user documentation. This reminded me that I need to flesh out all the machine-specific pages pointing users to where they can find the diagnostic input datasets/grid files/map files/diagnostic code paths.